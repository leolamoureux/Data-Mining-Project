{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>claps</th>\n",
       "      <th>reading_time</th>\n",
       "      <th>link</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Justin Lee</td>\n",
       "      <td>8.3K</td>\n",
       "      <td>11</td>\n",
       "      <td>https://medium.com/swlh/chatbots-were-the-next...</td>\n",
       "      <td>Chatbots were the next big thing: what happene...</td>\n",
       "      <td>Oh, how the headlines blared:\\nChatbots were T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Conor Dewey</td>\n",
       "      <td>1.4K</td>\n",
       "      <td>7</td>\n",
       "      <td>https://towardsdatascience.com/python-for-data...</td>\n",
       "      <td>Python for Data Science: 8 Concepts You May Ha...</td>\n",
       "      <td>If you’ve ever found yourself looking up the s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>William Koehrsen</td>\n",
       "      <td>2.8K</td>\n",
       "      <td>11</td>\n",
       "      <td>https://towardsdatascience.com/automated-featu...</td>\n",
       "      <td>Automated Feature Engineering in Python – Towa...</td>\n",
       "      <td>Machine learning is increasingly moving from h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gant Laborde</td>\n",
       "      <td>1.3K</td>\n",
       "      <td>7</td>\n",
       "      <td>https://medium.freecodecamp.org/machine-learni...</td>\n",
       "      <td>Machine Learning: how to go from Zero to Hero ...</td>\n",
       "      <td>If your understanding of A.I. and Machine Lear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Emmanuel Ameisen</td>\n",
       "      <td>935</td>\n",
       "      <td>11</td>\n",
       "      <td>https://blog.insightdatascience.com/reinforcem...</td>\n",
       "      <td>Reinforcement Learning from scratch – Insight ...</td>\n",
       "      <td>Want to learn about applied Artificial Intelli...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             author claps  reading_time  \\\n",
       "0        Justin Lee  8.3K            11   \n",
       "1       Conor Dewey  1.4K             7   \n",
       "2  William Koehrsen  2.8K            11   \n",
       "3      Gant Laborde  1.3K             7   \n",
       "4  Emmanuel Ameisen   935            11   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://medium.com/swlh/chatbots-were-the-next...   \n",
       "1  https://towardsdatascience.com/python-for-data...   \n",
       "2  https://towardsdatascience.com/automated-featu...   \n",
       "3  https://medium.freecodecamp.org/machine-learni...   \n",
       "4  https://blog.insightdatascience.com/reinforcem...   \n",
       "\n",
       "                                               title  \\\n",
       "0  Chatbots were the next big thing: what happene...   \n",
       "1  Python for Data Science: 8 Concepts You May Ha...   \n",
       "2  Automated Feature Engineering in Python – Towa...   \n",
       "3  Machine Learning: how to go from Zero to Hero ...   \n",
       "4  Reinforcement Learning from scratch – Insight ...   \n",
       "\n",
       "                                                text  \n",
       "0  Oh, how the headlines blared:\\nChatbots were T...  \n",
       "1  If you’ve ever found yourself looking up the s...  \n",
       "2  Machine learning is increasingly moving from h...  \n",
       "3  If your understanding of A.I. and Machine Lear...  \n",
       "4  Want to learn about applied Artificial Intelli...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"articles.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'inputs avant suppression des textes non anglais : 337\n",
      "Nombre d'inputs après suppression des textes non anglais : 331\n",
      "Temps d'exécution : 0.9834677919998285 secondes\n"
     ]
    }
   ],
   "source": [
    "import langdetect\n",
    "\n",
    "def detect_language(text):\n",
    "    return langdetect.detect(text)\n",
    "\n",
    "df = pd.read_csv('articles.csv')\n",
    "print(\"Nombre d'inputs avant suppression des textes non anglais : \" + str(len(df)))\n",
    "\n",
    "# créer une liste vide qui contiendra les indices des lignes à supprimer\n",
    "indexes_to_delete = []\n",
    "\n",
    "# parcourir chaque ligne du dataframe\n",
    "for index, row in df.iterrows():\n",
    "\n",
    "    lang = detect_language(row['text'][0:200])\n",
    "    # si la valeur de l'attribut 'text' commence par un 'm'\n",
    "    if lang != 'en':\n",
    "    # ajouter l'index de cette ligne à la liste des indices à supprimer\n",
    "        indexes_to_delete.append(index)\n",
    "\n",
    "# supprimer les lignes du dataframe en utilisant la liste des indices à supprimer\n",
    "df.drop(indexes_to_delete, inplace=True)\n",
    "\n",
    "print(\"Nombre d'inputs après suppression des textes non anglais : \" + str(len(df)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPROCESSING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE PUNCTUATION\n",
    "import string\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    punctuationfree=\"\".join([i for i in text if i not in string.punctuation])\n",
    "    return punctuationfree\n",
    "\n",
    "df[\"text\"] = df[\"text\"].apply(lambda row :remove_punctuation(row))\n",
    "df[\"text\"] = df[\"text\"].apply(lambda row: row.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZATION\n",
    "import nltk\n",
    "import nltk.corpus\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x : word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/leolamoureux/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# REMOVE STOP WORDS\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x: [w for w in x if not w in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/leolamoureux/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# LEMMATIZATION\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x: [WordNetLemmatizer().lemmatize(w) for w in x])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = set()\n",
    "\n",
    "# Iterate over each list of words\n",
    "for words in df[\"text\"].values.tolist():\n",
    "# Add the unique words from this list to the set\n",
    "    for word in words:\n",
    "        unique_words.add(word)\n",
    "\n",
    "# Convert the set of unique words back into a list\n",
    "unique_words = list(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_count_dataframe(words, lists_of_words):\n",
    "  # Create an empty data frame with the columns for each word\n",
    "  df = pd.DataFrame(columns=words)\n",
    "\n",
    "  # Create a list to store the rows of word counts\n",
    "  rows = []\n",
    "\n",
    "  # Iterate over each list of words\n",
    "  for words_list in lists_of_words:\n",
    "    # Create a dictionary with the word counts for this list of words\n",
    "    word_counts = {}\n",
    "    for word in words:\n",
    "      word_counts[word] = words_list.count(word)\n",
    "\n",
    "    # Add the word counts for this list of words to the list of rows\n",
    "    rows.append(word_counts)\n",
    "\n",
    "  # Concatenate the rows into the data frame\n",
    "  df = pd.concat([df, pd.DataFrame(rows)], ignore_index=True)\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    oscillating registered ygl b1 128x128 endeavour clay manager cig  \\\n",
      "0             0          0   0  0       0         0    0       0   0   \n",
      "1             0          0   0  0       0         0    0       0   0   \n",
      "2             0          0   0  0       0         0    0       0   0   \n",
      "3             0          0   0  0       0         0    0       0   0   \n",
      "4             0          0   0  0       0         0    0       0   0   \n",
      "..          ...        ...  .. ..     ...       ...  ...     ...  ..   \n",
      "326           0          0   0  0       0         0    0       0   0   \n",
      "327           0          0   0  0       0         0    0       0   0   \n",
      "328           0          0   0  0       0         0    0       0   0   \n",
      "329           0          0   0  0       0         0    0       0   0   \n",
      "330           0          0   0  0       0         0    0       0   0   \n",
      "\n",
      "    eyecatching  ... 025 resonate envision delf sticky contacted confronted  \\\n",
      "0             0  ...   0        0        0    0      0         0          0   \n",
      "1             0  ...   0        0        0    0      0         0          0   \n",
      "2             0  ...   0        0        0    0      0         0          0   \n",
      "3             0  ...   0        0        0    0      0         0          0   \n",
      "4             0  ...   0        0        0    0      0         0          0   \n",
      "..          ...  ...  ..      ...      ...  ...    ...       ...        ...   \n",
      "326           0  ...   0        0        0    1      0         0          0   \n",
      "327           0  ...   0        0        0    0      0         0          0   \n",
      "328           0  ...   0        0        0    0      0         0          0   \n",
      "329           0  ...   0        0        0    0      0         0          0   \n",
      "330           0  ...   0        0        0    0      0         0          0   \n",
      "\n",
      "    element tail defensible  \n",
      "0         1    0          0  \n",
      "1         4    0          0  \n",
      "2         1    0          0  \n",
      "3         0    0          0  \n",
      "4         0    0          0  \n",
      "..      ...  ...        ...  \n",
      "326       0    0          0  \n",
      "327       0    0          0  \n",
      "328       0    0          0  \n",
      "329       0    0          0  \n",
      "330       0    2          0  \n",
      "\n",
      "[331 rows x 17974 columns]\n"
     ]
    }
   ],
   "source": [
    "df_words = create_word_count_dataframe(unique_words, df[\"text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
