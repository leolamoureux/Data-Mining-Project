{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>claps</th>\n",
       "      <th>reading_time</th>\n",
       "      <th>link</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Justin Lee</td>\n",
       "      <td>8.3K</td>\n",
       "      <td>11</td>\n",
       "      <td>https://medium.com/swlh/chatbots-were-the-next...</td>\n",
       "      <td>Chatbots were the next big thing: what happene...</td>\n",
       "      <td>Oh, how the headlines blared:\\nChatbots were T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Conor Dewey</td>\n",
       "      <td>1.4K</td>\n",
       "      <td>7</td>\n",
       "      <td>https://towardsdatascience.com/python-for-data...</td>\n",
       "      <td>Python for Data Science: 8 Concepts You May Ha...</td>\n",
       "      <td>If you’ve ever found yourself looking up the s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>William Koehrsen</td>\n",
       "      <td>2.8K</td>\n",
       "      <td>11</td>\n",
       "      <td>https://towardsdatascience.com/automated-featu...</td>\n",
       "      <td>Automated Feature Engineering in Python – Towa...</td>\n",
       "      <td>Machine learning is increasingly moving from h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gant Laborde</td>\n",
       "      <td>1.3K</td>\n",
       "      <td>7</td>\n",
       "      <td>https://medium.freecodecamp.org/machine-learni...</td>\n",
       "      <td>Machine Learning: how to go from Zero to Hero ...</td>\n",
       "      <td>If your understanding of A.I. and Machine Lear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Emmanuel Ameisen</td>\n",
       "      <td>935</td>\n",
       "      <td>11</td>\n",
       "      <td>https://blog.insightdatascience.com/reinforcem...</td>\n",
       "      <td>Reinforcement Learning from scratch – Insight ...</td>\n",
       "      <td>Want to learn about applied Artificial Intelli...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             author claps  reading_time  \\\n",
       "0        Justin Lee  8.3K            11   \n",
       "1       Conor Dewey  1.4K             7   \n",
       "2  William Koehrsen  2.8K            11   \n",
       "3      Gant Laborde  1.3K             7   \n",
       "4  Emmanuel Ameisen   935            11   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://medium.com/swlh/chatbots-were-the-next...   \n",
       "1  https://towardsdatascience.com/python-for-data...   \n",
       "2  https://towardsdatascience.com/automated-featu...   \n",
       "3  https://medium.freecodecamp.org/machine-learni...   \n",
       "4  https://blog.insightdatascience.com/reinforcem...   \n",
       "\n",
       "                                               title  \\\n",
       "0  Chatbots were the next big thing: what happene...   \n",
       "1  Python for Data Science: 8 Concepts You May Ha...   \n",
       "2  Automated Feature Engineering in Python – Towa...   \n",
       "3  Machine Learning: how to go from Zero to Hero ...   \n",
       "4  Reinforcement Learning from scratch – Insight ...   \n",
       "\n",
       "                                                text  \n",
       "0  Oh, how the headlines blared:\\nChatbots were T...  \n",
       "1  If you’ve ever found yourself looking up the s...  \n",
       "2  Machine learning is increasingly moving from h...  \n",
       "3  If your understanding of A.I. and Machine Lear...  \n",
       "4  Want to learn about applied Artificial Intelli...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"articles.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'inputs avant suppression des textes non anglais : 337\n",
      "Nombre d'inputs après suppression des textes non anglais : 331\n"
     ]
    }
   ],
   "source": [
    "import langdetect\n",
    "\n",
    "def detect_language(text):\n",
    "    return langdetect.detect(text)\n",
    "\n",
    "df = pd.read_csv('articles.csv')\n",
    "print(\"Nombre d'inputs avant suppression des textes non anglais : \" + str(len(df)))\n",
    "\n",
    "# créer une liste vide qui contiendra les indices des lignes à supprimer\n",
    "indexes_to_delete = []\n",
    "\n",
    "# parcourir chaque ligne du dataframe\n",
    "for index, row in df.iterrows():\n",
    "\n",
    "    lang = detect_language(row['text'][0:200])\n",
    "    # si la valeur de l'attribut 'text' commence par un 'm'\n",
    "    if lang != 'en':\n",
    "    # ajouter l'index de cette ligne à la liste des indices à supprimer\n",
    "        indexes_to_delete.append(index)\n",
    "\n",
    "# supprimer les lignes du dataframe en utilisant la liste des indices à supprimer\n",
    "df.drop(indexes_to_delete, inplace=True)\n",
    "\n",
    "print(\"Nombre d'inputs après suppression des textes non anglais : \" + str(len(df)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPROCESSING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE PUNCTUATION\n",
    "import string\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    punctuationfree=\"\".join([i for i in text if i not in string.punctuation])\n",
    "    return punctuationfree\n",
    "\n",
    "df[\"text\"] = df[\"text\"].apply(lambda row :remove_punctuation(row))\n",
    "df[\"text\"] = df[\"text\"].apply(lambda row: row.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZATION\n",
    "import nltk\n",
    "import nltk.corpus\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x : word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/leolamoureux/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# REMOVE STOP WORDS\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x: [w for w in x if not w in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/leolamoureux/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# LEMMATIZATION\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x: [WordNetLemmatizer().lemmatize(w) for w in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import enchant\n",
    "\n",
    "# Create a UK English dictionary using enchant\n",
    "dictionary = enchant.Dict(\"en_GB\")\n",
    "\n",
    "# Define a regular expression pattern to match non-alphabetic characters\n",
    "pattern = re.compile(r'[^a-zA-Z]')\n",
    "\n",
    "# Remove all non-English words from the documents\n",
    "filtered_documents = []\n",
    "for doc in df[\"text\"]:\n",
    "    filtered_doc = []\n",
    "    for word in doc:\n",
    "        # Remove non-alphabetic characters from the word\n",
    "        word = pattern.sub('', word)\n",
    "        # Check if the word is in the English dictionary\n",
    "        if word and dictionary.check(word):\n",
    "            filtered_doc.append(word)\n",
    "    filtered_documents.append(filtered_doc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = set()\n",
    "\n",
    "# Iterate over each list of words\n",
    "for words in filtered_documents:\n",
    "# Add the unique words from this list to the set\n",
    "    for word in words:\n",
    "        unique_words.add(word)\n",
    "\n",
    "# Convert the set of unique words back into a list\n",
    "unique_words = list(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "#optimize\n",
    "def create_word_count_dataframe(words, lists_of_words):\n",
    "  # Create an empty data frame with the columns for each word\n",
    "  df = pd.DataFrame(columns=words)\n",
    "\n",
    "  # Create a list to store the rows of word counts\n",
    "  rows = []\n",
    "\n",
    "  # Iterate over each list of words\n",
    "  for words_list in lists_of_words:\n",
    "    # Use the Counter class to count the occurrences of each word\n",
    "    word_counts = dict(Counter(words_list))\n",
    "\n",
    "    # Ensure that the word_counts dictionary includes a count for every word\n",
    "    row = {word: word_counts.get(word, 0) for word in words}\n",
    "\n",
    "    # Add the word counts for this list of words to the list of rows\n",
    "    rows.append(row)\n",
    "\n",
    "  # Concatenate the rows into the data frame\n",
    "  df = pd.concat([df, pd.DataFrame(rows)], ignore_index=True)\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_words_count = create_word_count_dataframe(unique_words, df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "def create_tfidf_dataframe(df):\n",
    "  # Create a new data frame with the same index as the original data frame (i.e., the words)\n",
    "  tfidf_df = pd.DataFrame(index=df.columns)\n",
    "\n",
    "  # Add a column for the total number of documents (i.e., the number of rows in the data frame)\n",
    "  #tfidf_df['tfidf'] = df.shape[0]\n",
    "\n",
    "  # Add a column for the document frequency of each word\n",
    "  tfidf_df['df'] = (df > 0).sum(axis=0)\n",
    "\n",
    "  # Add a column for the inverse document frequency of each word\n",
    "  tfidf_df['idf'] = tfidf_df['df'].apply(lambda x: log(df.shape[0] / x))\n",
    "\n",
    "  # Add a column for the TF-IDF of each word\n",
    "  tfidf_df['tfidf'] = tfidf_df['df'] * tfidf_df['idf']\n",
    "\n",
    "  return tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 df       idf       tfidf\n",
      "tweaked           4  4.415824   17.663296\n",
      "cocktail          4  4.415824   17.663296\n",
      "brightly          1  5.802118    5.802118\n",
      "responds          3  4.703506   14.110518\n",
      "conceptualizes    1  5.802118    5.802118\n",
      "...             ...       ...         ...\n",
      "peering           1  5.802118    5.802118\n",
      "constantly       28  2.469914   69.157588\n",
      "honeymoon         2  5.108971   10.217942\n",
      "actually        146  0.818512  119.502716\n",
      "godfather         5  4.192680   20.963402\n",
      "\n",
      "[17974 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(create_tfidf_dataframe(df_words_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     aah   ab  aback  abandoned  abbreviated  abilities   ability  ablation  \\\n",
      "0    0.0  0.0    0.0        0.0          0.0   0.000000  0.000000       0.0   \n",
      "1    0.0  0.0    0.0        0.0          0.0   0.000000  0.019030       0.0   \n",
      "2    0.0  0.0    0.0        0.0          0.0   0.000000  0.000000       0.0   \n",
      "3    0.0  0.0    0.0        0.0          0.0   0.000000  0.000000       0.0   \n",
      "4    0.0  0.0    0.0        0.0          0.0   0.000000  0.000000       0.0   \n",
      "..   ...  ...    ...        ...          ...        ...       ...       ...   \n",
      "326  0.0  0.0    0.0        0.0          0.0   0.000000  0.000000       0.0   \n",
      "327  0.0  0.0    0.0        0.0          0.0   0.052112  0.030109       0.0   \n",
      "328  0.0  0.0    0.0        0.0          0.0   0.000000  0.026217       0.0   \n",
      "329  0.0  0.0    0.0        0.0          0.0   0.000000  0.000000       0.0   \n",
      "330  0.0  0.0    0.0        0.0          0.0   0.000000  0.000000       0.0   \n",
      "\n",
      "         able  abnormal  ...  zeroth  zest  zigzag  zip  zipped  zombies  \\\n",
      "0    0.010657       0.0  ...     0.0   0.0     0.0  0.0     0.0      0.0   \n",
      "1    0.000000       0.0  ...     0.0   0.0     0.0  0.0     0.0      0.0   \n",
      "2    0.000000       0.0  ...     0.0   0.0     0.0  0.0     0.0      0.0   \n",
      "3    0.030177       0.0  ...     0.0   0.0     0.0  0.0     0.0      0.0   \n",
      "4    0.000000       0.0  ...     0.0   0.0     0.0  0.0     0.0      0.0   \n",
      "..        ...       ...  ...     ...   ...     ...  ...     ...      ...   \n",
      "326  0.025798       0.0  ...     0.0   0.0     0.0  0.0     0.0      0.0   \n",
      "327  0.051504       0.0  ...     0.0   0.0     0.0  0.0     0.0      0.0   \n",
      "328  0.000000       0.0  ...     0.0   0.0     0.0  0.0     0.0      0.0   \n",
      "329  0.000000       0.0  ...     0.0   0.0     0.0  0.0     0.0      0.0   \n",
      "330  0.018426       0.0  ...     0.0   0.0     0.0  0.0     0.0      0.0   \n",
      "\n",
      "         zone  zoo  zoologist  zoom  \n",
      "0    0.034521  0.0        0.0   0.0  \n",
      "1    0.000000  0.0        0.0   0.0  \n",
      "2    0.000000  0.0        0.0   0.0  \n",
      "3    0.000000  0.0        0.0   0.0  \n",
      "4    0.000000  0.0        0.0   0.0  \n",
      "..        ...  ...        ...   ...  \n",
      "326  0.000000  0.0        0.0   0.0  \n",
      "327  0.000000  0.0        0.0   0.0  \n",
      "328  0.000000  0.0        0.0   0.0  \n",
      "329  0.000000  0.0        0.0   0.0  \n",
      "330  0.000000  0.0        0.0   0.0  \n",
      "\n",
      "[331 rows x 12788 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create a TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "documents = [' '.join(doc) for doc in filtered_documents]\n",
    "\n",
    "# Fit the vectorizer on the documents and transform them into TF-IDF vectors\n",
    "tfidf = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get the feature names (words)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "# Create a DataFrame from the TF-IDF vectors\n",
    "tfidf_df = pd.DataFrame(tfidf.toarray(), columns=feature_names)\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(tfidf_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
